#!/bin/bash

set -exo pipefail

PROJECT="moz-fx-data-backfill-2"
JOB_NAME="minidump-backfill"

## this script assumes it's being run from the ingestion-beam directory
## of the gcp-ingestion repo.

mvn compile exec:java -Dexec.mainClass=com.mozilla.telemetry.Decoder -Dexec.args="\
    --runner=Dataflow \
    --jobName=$JOB_NAME \
    --project=$PROJECT  \
    --geoCityDatabase=gs://backfill-test-public1/GeoIP2-City.mmdb \
    --geoCityFilter=gs://backfill-test-public1/cities15000.txt \
    --schemasLocation=gs://backfill-test-public1/202001101955_c5b19a4.tar.gz \
    --inputType=bigquery_query \
    --input=\"with pings as (SELECT \`moz-fx-data-shared-prod\`.udf.parse_desktop_telemetry_uri(uri).document_id, * FROM \`moz-fx-data-shared-prod.payload_bytes_error.telemetry\` WHERE DATE(submission_timestamp) between '2019-12-20' and '2020-01-08' AND document_type = 'crash' AND payload is not null and (error_message LIKE 'org.everit.json.schema.ValidationException: #/payload/minidumpSha256Hash%')), distinct_document_ids AS (SELECT document_id, MIN(submission_timestamp) AS submission_timestamp FROM pings GROUP BY document_id), base AS (SELECT * FROM pings JOIN distinct_document_ids USING (document_id, submission_timestamp)), numbered_duplicates AS (SELECT *, ROW_NUMBER() OVER (PARTITION BY document_id) AS _n FROM base) SELECT * FROM numbered_duplicates WHERE _n = 1\" \
    --bqReadMethod=export \
    --outputType=bigquery \
    --bqWriteMethod=file_loads \
    --bqClusteringFields=normalized_channel,sample_id \
    --output=${PROJECT}:\${document_namespace}_stable.\${document_type}_v\${document_version} \
    --errorOutputType=bigquery \
    --errorOutput=${PROJECT}:payload_bytes_error.telemetry \
    --experiments=shuffle_mode=service \
    --region=us-central1 \
    --usePublicIps=false \
    --gcsUploadBufferSizeBytes=16777216 \
"
